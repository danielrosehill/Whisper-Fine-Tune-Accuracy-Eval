{
  "audio_file": "042_local_tools",
  "model": "finetune-tiny",
  "reference": "I converted the model to GGML format for faster CPU inference. The quantized version loads in seconds and uses much less RAM than the original.",
  "hypothesis": "I converted the model to GDML format for faster CPU inference. The quantized version loads in seconds and uses much less RAM than the original.",
  "reference_normalized": "i converted the model to ggml format for faster cpu inference the quantized version loads in seconds and uses much less ram than the original",
  "hypothesis_normalized": "i converted the model to gdml format for faster cpu inference the quantized version loads in seconds and uses much less ram than the original",
  "wer": 0.04,
  "duration_seconds": 0.2554318904876709
}