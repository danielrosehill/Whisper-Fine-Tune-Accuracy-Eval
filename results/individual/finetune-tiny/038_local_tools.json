{
  "audio_file": "038_local_tools",
  "model": "finetune-tiny",
  "reference": "Ollama is running on port eleven thousand four hundred thirty four. I pulled the latest Llama model and it works great for local inference.",
  "hypothesis": "O'Lama is running on port 11,434. I pull the latest Lama model and it works great for local inference.",
  "reference_normalized": "ollama is running on port 11434 i pulled the latest llama model and it works great for local inference",
  "hypothesis_normalized": "0 lama is running on port 11434 i pull the latest lama model and it works great for local inference",
  "wer": 0.21052631578947367,
  "duration_seconds": 0.24124646186828613
}