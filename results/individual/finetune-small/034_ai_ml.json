{
  "audio_file": "034_ai_ml",
  "model": "finetune-small",
  "reference": "The inference speed depends on batch size and model quantization. Using four bit quantization cuts memory usage in half with minimal impact on output quality.",
  "hypothesis": "The inference speed depends on batch size and model quantization. Using 4-bit quantization cuts memory usage in half with minimal impact on output quality.",
  "reference_normalized": "the inference speed depends on batch size and model quantization using 4 bit quantization cuts memory usage in half with minimal impact on output quality",
  "hypothesis_normalized": "the inference speed depends on batch size and model quantization using 4 bit quantization cuts memory usage in half with minimal impact on output quality",
  "wer": 0.0,
  "duration_seconds": 0.5323312282562256
}